---
title: "Trabajo 02 de R"
author: "Carolina Yépez Castillo"
date: "Agosto 02, 2015"
output: word_document
---
1.- Realice un Fork del repositorio Regresion-Lineal-Multiple.

2.-  Genere un archivo rlm.R, de tal forma que le permita responder las siguientes preguntas:

2.1.- Leer los archivos poblacion1.xlsx y poblacion2.xlsx , y analizar sus dimensiones.

```{r,echo=FALSE,eval=TRUE}
library(readxl)
poblacion1<-read_excel("poblacion1.xlsx", sheet=1, na=" ")

poblacion2<-read_excel("poblacion2.xlsx", sheet=1, na=" ")

View(poblacion1)
#str(poblacion1)
names(poblacion1)

View(poblacion2)
#str(poblacion2)
names(poblacion2)
```
Analizando las dimensiones de cada archivo

Se puede observar que poblacion1 tiene:
```{r,echo=FALSE,eval=TRUE}
nrow(poblacion1)
```
observaciones

```{r,echo=FALSE,eval=TRUE}
ncol(poblacion1) 
```
variables

Así, mismo poblacion2 tiene:

```{r,echo=FALSE,eval=TRUE}
nrow(poblacion2)
```
observaciones
```{r,echo=FALSE,eval=TRUE}
ncol(poblacion2)
```
variables

2.2.- Una los archivos leídos en un mismo objeto llamado población.

Uso la funcion merge para unir dos data frames:

```{r,echo=FALSE,eval=TRUE}
poblacion <- merge(poblacion1,poblacion2,by="identificador",suffixes=c("",""))
poblacion
View(poblacion)
```

2.3.- Cree un código que identiﬁque la clase de cada variable y genere diagramas de cajas para variables continuas y diagramas de barras para variables discretas.

Para variables continuas:

```{r,echo=FALSE,eval=TRUE}
for(j in 2:dim(poblacion)[2]){
  if(is.numeric(poblacion[,j])==TRUE){
    boxplot(poblacion[,j])
    title(names(poblacion)[j])
  }
}

```


Para variables discretas:

```{r,echo=FALSE,eval=TRUE}
for(j in 2:dim(poblacion)[2]){
  if(is.numeric(poblacion[,j])!=TRUE){
    barplot(table(poblacion[,j]))
    title(names(poblacion)[j])
  }
}
```

2.3.- Cree un código que calcule automáticamente el mínimo, media, máximo, desviación estándar, primer cuartil de cada variable numérica y la frecuencia en el caso de variables categóricas.

```{r,echo=FALSE,eval=TRUE}
tipo <- 1:dim(poblacion)[2]
maxi <- 1:dim(poblacion)[2]
mini <- 1:dim(poblacion)[2]
media <- 1:dim(poblacion)[2]
desviacion <- 1:dim(poblacion)[2]
primer_cuartil <- 1:dim(poblacion)[2]



for(j in 2:dim(poblacion)[2]){
      if(is.numeric(poblacion[,j])==TRUE){
          tipo[j] <- class(poblacion[,j])
          maxi[j] <- max(poblacion[,j])
          mini[j] <- min(poblacion[,j])
          media[j] <- mean(poblacion[,j])
          desviacion[j] <- sd(poblacion[,j])
          primer_cuartil[j] <- quantile(poblacion[,j],probs=seq(0,1,0.25),na.rm = FALSE)[2]
        }else {
            
            tipo[j] <- class(poblacion[,j])
            maxi[j] <- NA
            mini[j] <- NA
            media[j] <- NA
            desviacion[j] <- NA
            primer_cuartil[j] <- NA 
            table(poblacion[,j])/dim(poblacion)[1]
          }
}
```

Resultados:


```{r,echo=FALSE,eval=TRUE}
resultados1 <- data.frame(names(poblacion),tipo,maxi,mini,media,desviacion,primer_cuartil)
resultados1
View(resultados1)
```

Ahora considerando las variables categoricas "region" y "serv.bas.compl", hay que calcular las frecuencias:

```{r,echo=FALSE,eval=TRUE}
frecuencia_region  <- table(poblacion[,9])/dim(poblacion)[1]
frecuencia_ser.bas.compl <- table(poblacion[,10])/dim(poblacion)[1]

frecuencia_region
frecuencia_ser.bas.compl
```

2.3.- Calcule la correlación entre la variable dependiente poblacion y cada una de las variables explicativas (numéricas).

```{r,echo=FALSE,eval=TRUE}
correlacion <- 3:dim(poblacion)[2]
correlacion[1] <- NA


for(j in 2:dim(poblacion)[2]){
    if(is.numeric(poblacion[,j])==TRUE){
      correlacion[j] <- cor(poblacion[,2], poblacion[,j])
      } else {
          correlacion[j] <- NA
        }
  }
```

Ahora la correlación con cada variable será:

```{r,echo=FALSE,eval=TRUE}
resultados2 <- data.frame(names(poblacion),correlacion) 
resultados2
View(resultados2)
```

2.5.- Considere la variable categórica serv.bas.compl con una conﬁabilidad del 90%, ¿Puede asumirse que la media de la variable poblacion en el grupo serv.bas.compl: SI es distinta a la media del grupo serv.bas.compl: NO ?


Transformo a variable factor las columnas "region" y "serv.bas.compl" 
para poder usar el test t(student)

```{r,echo=FALSE,eval=TRUE}
serv.bas.compl_fac <- factor(poblacion[,"serv.bas.compl"],levels=c("SI","NO"),labels=c("si","no"))
region_fac <- factor(poblacion[,"region"],levels=c("A","B"),labels=c("a","b"))
poblacion_fac <- data.frame(poblacion[,1:7],region_fac,serv.bas.compl_fac)
poblacion_fac
```

Diagrama de cajas
```{r,echo=FALSE,eval=TRUE}
plot(poblacion ~ serv.bas.compl_fac , data = poblacion_fac) 
```

Por último realizamos la prueba de hipotesis: 
```{r,echo=FALSE,eval=TRUE}
pr_hipotesis <- t.test(poblacion ~ serv.bas.compl_fac , data=poblacion_fac, conf.level=0.9)
pr_hipotesis
```

Por lo tanto como conclusión, se acepta que la diferencia de medias es igual a cero.


2.6.- Considerando los cálculos anteriores genere el modelo de regresión lineal múltiple que mejor se ajuste a los datos. 
Interprete los coeﬁcientes obtenidos.



Hacemos en modelo de regresión lineal multiple con la variable independiente "poblacion", y como variables dependientes "var.pobl.mayor", "menores.18", "tasa.crimen" ya que fueron las que presentaron mayor correlación. 

```{r,echo=FALSE,eval=TRUE}
reg1 <- lm(poblacion~var.pobl.mayor+menores.18+tasa.crimen, data=poblacion_fac)
summary(reg1)
```

Se tiene que $\beta_{1}=7.388436$
$\beta_{2}=0.029694$
$\beta_{3}=0.039498$
$\beta_{4}=-0.012813$

Interpretemos los coeficientes del modelo: 

Si las variables: $\hat{`r substring(names(poblacion)[3],1)`}$, $\hat{`r substring(names(poblacion)[4],1)`}$ se mantienen constantes y la $\hat{`r substring(names(poblacion)[7],1)`}$ aumenta en una unidad (1%), se tiene que: 

la $\hat{`r substring(names(poblacion)[2],1)`}$ disminuye en promedio  `r reg1$coefficients[4]`  unidades  (`r reg1$coefficients[4]` %).

Si las variables:  $\hat{`r substring(names(poblacion)[3],1)`}$, $\hat{`r substring(names(poblacion)[7],1)`}$ se mantienen constantes y la $\hat{`r substring(names(poblacion)[4],1)`}$ aumenta en una unidad (1%), se tiene que: 

la $\hat{`r substring(names(poblacion)[2],1)`}$ aumenta en promedio  `r reg1$coefficients[3]`  unidades  (`r reg1$coefficients[3]` %).


Si las variables:  $\hat{`r substring(names(poblacion)[4],1)`}$, $\hat{`r substring(names(poblacion)[7],1)`}$ se mantienen constantes y la $\hat{`r substring(names(poblacion)[3],1)`}$ aumenta en una unidad (1%), se tiene que: 

la $\hat{`r substring(names(poblacion)[2],1)`}$ aumenta en promedio  `r reg1$coefficients[2]`  unidades  (`r reg1$coefficients[2]` %).

2.7.- Interprete el R2

Analizando la regresión se tuvo un $R^{2}$: 
```{r,echo=FALSE,eval=TRUE}
R2 <- summary(reg1)[["r.squared"]]
```
Así se tiene como respuesta que $R^{2}= 0.1533239$

Esto quiere decir que la regresión explica el:
```{r,echo=FALSE,eval=TRUE}
porcentaje <- 100*summary(reg1)[["r.squared"]]
```
porcentaje=$15.33%$ de la variabilidad de la regresión.

2.8.-  Analice la signiﬁcancia de la regresión y de cada uno de los 
parámetros individuales. 

```{r,echo=FALSE,eval=TRUE}
anova <- aov(reg1)
summary(anova)
```
Se tiene que $F=2,173$ mientras que el F tableado es: 
```{r,echo=FALSE,eval=TRUE}
qf(0.90,df1=3,df2=36)

```
$F_{(3,36)}(\alpha)= 2.242605$
Como $F < F_{(3,36)}(\alpha)$ entonces la regresion no es significativa.

```{r,echo=FALSE,eval=TRUE}
qt(0.95,df=36)
```

2.9.-  Realice un análisis detallado de los residuos.

```{r,echo=FALSE,eval=TRUE}
residuos<-1:40  
for(i in 1:40){
    residuos[i]<-summary(reg1)[["residuals"]][i]
  }
```

poblacion vs residuos
```{r,echo=FALSE,eval=TRUE}
plot(poblacion[,"poblacion"],residuos)
```

histograma 
```{r,echo=FALSE,eval=TRUE}
hist(residuos)
```

```{r,echo=FALSE,eval=TRUE}
qqnorm(residuos)
qqline(residuos,col="red")
```

Como conclusiones, se tiene que creando esa regresión lineal múltiple con las variables que tienen mayor correlación, se acepta la hipotesis de que las medias son iguales. Además la regresion no es significativa.
Analizando el gráfico de probabilidad normal, se tiene que no rechaza la normalidad de los errores.



